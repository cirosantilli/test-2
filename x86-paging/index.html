<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
      
        x86 Paging Tutorial -
      
      Ciro Santilli
    </title>
    
      <meta name="description" content="With simple examples and applications.">
    
    <link rel="shortcut icon" type="image/x-icon" href="https://www.gravatar.com/avatar/c979bd881b6755f84947a5a8ecc7c6f7.png" />
    <!-- #jquery. Must come *before* all its many dependencies. -->
      <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
    <!-- #bootstrap -->
      <link href="https://netdna.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css" rel="stylesheet"/>
      <script src="https://netdna.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js"></script>
    <!-- #data table -->
      <link rel="stylesheet" href="https://ajax.aspnetcdn.com/ajax/jquery.dataTables/1.9.4/css/jquery.dataTables.css"/>
      <style>
        table.dataTable tr.odd { background-color: white; }
        table.dataTable tr.even { background-color: white; }
        table.dataTable tr.odd td.sorting_1 { background-color: white; }
        table.dataTable tr.odd td.sorting_2 { background-color: white; }
        table.dataTable tr.odd td.sorting_3 { background-color: white; }
        table.dataTable tr.even td.sorting_1 { background-color: white; }
        table.dataTable tr.even td.sorting_2 { background-color: white; }
        table.dataTable tr.even td.sorting_3 { background-color: white; }
      </style>
    <!-- #font awesome font icons -->
      <link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet"/>
    <!-- Custom styles and scripts. Come at end to use and override libraries. -->
      <!--<script src="/assets/main.js"></script>-->
      <link rel="stylesheet" href="/assets/main.css"/>
      <link rel="stylesheet" href="/assets/header-link.css"/>
      <link rel="stylesheet" href="/assets/syntax.css"/>
  </head>
  <body>
    <header>
      <nav class="navbar navbar-default" role="navigation">
        <div class="container-fluid">
          <div class="navbar-header ">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Ciro Santilli</a>
          </div>
          <div class="collapse navbar-collapse" id="navbar">
            <ul class="nav navbar-nav">
              
              <li >
                <a href="/articles" title="">Articles</a>
              </li>
              
              <li >
                <a href="/projects" title="">Projects</a>
              </li>
              
              <li >
                <a href="/skills" title="">Skills</a>
              </li>
              
              <li >
                <a href="/interests" title="">Interests</a>
              </li>
              
              <li >
                <a href="/education" title="">Education</a>
              </li>
              
            </ul>
          </div>
        </div>
      </nav>
    </header>
    <div class="article-and-sidebar">
      <div class="content">
        <aside>
          <div>
            <a href="/">
              <img style="width:100%;" src="/assets/me.jpg" alt="">
            </a>
          </div>
          <div>
            <a class="nav" href="https://stackoverflow.com/users/895245"><i class="fa fa-stack-overflow fa-fw"></i>Stack Overflow</a>
            <a class="nav" href="https://github.com/cirosantilli"><i class="fa fa-github fa-fw"></i>GitHub</a>
            <a class="nav" href="https://www.linkedin.com/in/cirosantilli"><i class="fa fa-linkedin-square fa-fw"></i>LinkedIn</a>
            <a class="nav" href="https://www.youtube.com/channel/UCBrJjpKMpdrkA1VsAiR2wEg"><i class="fa fa-youtube fa-fw"></i>YouTube</a>
            <a class="nav" href="https://twitter.com/cirosantilli"><i class="fa fa-twitter fa-fw"></i>Twitter</a>
            
            <a class="nav" href="https://www.zhihu.com/people/cirosantilli/activities"><i class="fa fa-question-circle fa-fw"></i>Zhihu 知乎</a>
            <a class="nav" href="https://www.weibo.com/p/1005055601627311"><i class="fa fa-weibo fa-fw"></i>Weibo 微博</a>
            
            <a class="nav" href="/accounts"><i class="fa fa-ellipsis-h fa-fw"></i>Other accounts</a>
          </div>
          <div>
            
            
            
            
            
            <a class="nav" href="https://github.com/cirosantilli/cirosantilli.github.io/issues/new?title=x86+Paging+Tutorial%20-%20">
              <i class="fa fa-exclamation-circle fa-fw"></i>Report Issue
            </a>
          </div>
        </aside>
        <main>
          <h1 class="page-title">x86 Paging Tutorial</h1>
          
          <div id="sharethis">
            <!-- #ShareThis share -->
              <!-- #INVALID HTML5: https://getsatisfaction.com/sharethis/topics/attribute_displaytext_is_not_valid_xhtml -->
              <div class='st_twitter_hcount desktop' displayText='Tweet'></div>
              <div class='st_facebook_hcount desktop' displayText='Facebook'></div>
              <div class='st_googleplus_hcount desktop' displayText='Google +'></div>
              <div class='st_linkedin_hcount desktop' displayText='LinkedIn'></div>
              <div class='st_twitter_large mobile' displayText='Tweet'></div>
              <div class='st_facebook_large mobile' displayText='Facebook'></div>
              <div class='st_googleplus_large mobile' displayText='Google +'></div>
              <div class='st_linkedin_large mobile' displayText='LinkedIn'></div>
              <!--<span class='st_linkedin_hcount' displayText='LinkedIn'></span>-->
              <script type="text/javascript" src="http://w.sharethis.com/button/buttons.js"></script>
              <script type="text/javascript">stLight.options({publisher: "b6a81f16-5bb1-46de-83c8-6b4c3e94ec99", doNotHash:false, doNotCopy:true, hashAddressBar:false});</script>
          </div>
          
          <p>With simple examples and applications.</p>
          <p>Extracted and expanded from <a href="https://stackoverflow.com/a/18431262/895245">my Stack Overflow answer</a>.</p>

<ol id="markdown-toc">
  <li><a href="#sample-code" id="markdown-toc-sample-code">Sample code</a></li>
  <li><a href="#intel-manual" id="markdown-toc-intel-manual">Intel manual</a></li>
  <li><a href="#application" id="markdown-toc-application">Application</a></li>
  <li><a href="#hardware-implementation" id="markdown-toc-hardware-implementation">Hardware implementation</a></li>
  <li><a href="#segmentation" id="markdown-toc-segmentation">Segmentation</a></li>
  <li><a href="#example-simplified-single-level-paging-scheme" id="markdown-toc-example-simplified-single-level-paging-scheme">Example: simplified single-level paging scheme</a>    <ol>
      <li><a href="#single-level-paging-scheme-visualization" id="markdown-toc-single-level-paging-scheme-visualization">Single level paging scheme visualization</a></li>
      <li><a href="#single-level-paging-scheme-numerical-translation-example" id="markdown-toc-single-level-paging-scheme-numerical-translation-example">Single level paging scheme numerical translation example</a></li>
      <li><a href="#multiple-addresses-translate-to-a-single-physical-address" id="markdown-toc-multiple-addresses-translate-to-a-single-physical-address">Multiple addresses translate to a single physical address</a></li>
      <li><a href="#identity-mapping" id="markdown-toc-identity-mapping">Identity mapping</a></li>
      <li><a href="#page-faults" id="markdown-toc-page-faults">Page faults</a></li>
      <li><a href="#page-table-entries" id="markdown-toc-page-table-entries">Page table entries</a></li>
      <li><a href="#page-size-choice" id="markdown-toc-page-size-choice">Page size choice</a></li>
    </ol>
  </li>
  <li><a href="#example-multi-level-paging-scheme" id="markdown-toc-example-multi-level-paging-scheme">Example: multi-level paging scheme</a>    <ol>
      <li><a href="#the-problem-with-single-level-paging" id="markdown-toc-the-problem-with-single-level-paging">The problem with single-level paging</a></li>
      <li><a href="#k-ary-trees-to-the-rescue" id="markdown-toc-k-ary-trees-to-the-rescue">K-ary trees to the rescue</a></li>
      <li><a href="#why-not-a-balanced-tree" id="markdown-toc-why-not-a-balanced-tree">Why not a balanced tree</a></li>
      <li><a href="#how-the-k-ary-tree-is-used-in-x86" id="markdown-toc-how-the-k-ary-tree-is-used-in-x86">How the K-ary tree is used in x86</a></li>
      <li><a href="#multi-level-paging-scheme-numerical-translation-example" id="markdown-toc-multi-level-paging-scheme-numerical-translation-example">Multi-level paging scheme numerical translation example</a></li>
    </ol>
  </li>
  <li><a href="#64-bit-architectures" id="markdown-toc-64-bit-architectures">64-bit architectures</a></li>
  <li><a href="#pae" id="markdown-toc-pae">PAE</a></li>
  <li><a href="#pse" id="markdown-toc-pse">PSE</a></li>
  <li><a href="#pae-and-pse-page-table-schemes" id="markdown-toc-pae-and-pse-page-table-schemes">PAE and PSE page table schemes</a></li>
  <li><a href="#tlb" id="markdown-toc-tlb">TLB</a>    <ol>
      <li><a href="#basic-operation" id="markdown-toc-basic-operation">Basic operation</a></li>
      <li><a href="#replacement-policy" id="markdown-toc-replacement-policy">Replacement policy</a></li>
      <li><a href="#cam" id="markdown-toc-cam">CAM</a></li>
      <li><a href="#invalidating-entries" id="markdown-toc-invalidating-entries">Invalidating entries</a></li>
    </ol>
  </li>
  <li><a href="#linux-kernel-usage" id="markdown-toc-linux-kernel-usage">Linux kernel usage</a>    <ol>
      <li><a href="#play-with-physical-addresses-in-linux" id="markdown-toc-play-with-physical-addresses-in-linux">Play with physical addresses in Linux</a></li>
      <li><a href="#kernel-vs-process-memory-layout" id="markdown-toc-kernel-vs-process-memory-layout">Kernel vs process memory layout</a></li>
      <li><a href="#process-memory-layout" id="markdown-toc-process-memory-layout">Process memory layout</a></li>
      <li><a href="#copy-on-write" id="markdown-toc-copy-on-write">Copy-on-write</a></li>
      <li><a href="#linux-source-tree" id="markdown-toc-linux-source-tree">Linux source tree</a></li>
    </ol>
  </li>
  <li><a href="#memory-management-unit" id="markdown-toc-memory-management-unit">Memory management unit</a></li>
  <li><a href="#second-level-address-translation" id="markdown-toc-second-level-address-translation">Second Level Address Translation</a></li>
  <li><a href="#other-architectures" id="markdown-toc-other-architectures">Other architectures</a>    <ol>
      <li><a href="#arm" id="markdown-toc-arm">ARM</a></li>
    </ol>
  </li>
  <li><a href="#bibliography" id="markdown-toc-bibliography">Bibliography</a></li>
</ol>

<h2 id="sample-code">Sample code</h2>

<p>Minimal example: <a href="https://github.com/cirosantilli/x86-bare-metal-examples/blob/5c672f73884a487414b3e21bd9e579c67cd77621/paging.S">https://github.com/cirosantilli/x86-bare-metal-examples/blob/5c672f73884a487414b3e21bd9e579c67cd77621/paging.S</a></p>

<p>Like everything else in programming, the only way to really understand this is to play with minimal examples.</p>

<p>What makes this a “hard” subject is that the minimal example is large because you need to make your own small OS.</p>

<h2 id="intel-manual">Intel manual</h2>

<p>Although it is impossible to understand without examples in mind, try to get familiar with the manuals as soon as possible.</p>

<p>Intel describes paging in the <a href="https://web.archive.org/web/20151025081259/http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-system-programming-manual-325384.pdf">Intel Manual Volume 3 System Programming Guide - 325384-056US September 2015</a> Chapter 4 “Paging”.</p>

<p>Specially interesting is Figure 4-4 “Formats of CR3 and Paging-Structure Entries with 32-Bit Paging”, which gives the key data structures.</p>

<h2 id="application">Application</h2>

<p>Paging makes it easier to compile and run two programs at the same time on a single computer.</p>

<p>For example, when you compile two programs, the compiler does not know if they are going to be running at the same time or not.</p>

<p>So nothing prevents it from using the same RAM address, say, <code class="highlighter-rouge">0x1234</code>, to store a global variable.</p>

<p>But if two programs use the same address and run at the same time, this is obviously going to break them!</p>

<p>Paging solves this problem beautifully by adding one degree of indirection:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(logical) ------------&gt; (physical)
             paging
</code></pre></div></div>

<p>Where:</p>

<ul>
  <li>
    <p>logical addresses are what userland programs see, e.g. the contents of <code class="highlighter-rouge">rsi</code> in <code class="highlighter-rouge">mov eax, [rsi]</code>.</p>

    <p>They are often called “virtual” addresses as well.</p>
  </li>
  <li>
    <p>physical addresses can be though of the values that go to physical RAM index wires.</p>

    <p>But keep in mind that this is not 100% true because of further indirections such as:</p>

    <ul>
      <li><a href="https://en.wikipedia.org/wiki/Memory-mapped_I/O">memory-mapped I/O regions</a></li>
      <li><a href="https://en.wikipedia.org/wiki/Multi-channel_memory_architecture">multi channel memory</a></li>
    </ul>
  </li>
</ul>

<p>Compilers don’t need to worry about other programs: they just use simple logical addresses.</p>

<p>As far as programs are concerned, they think they can use any address between 0 and 4GiB (2^32, <code class="highlighter-rouge">FFFFFFFF</code>) on 32-bit systems.</p>

<p>The OS then sets up paging so that identical logical addresses will go into different physical addresses and not overwrite each other.</p>

<p>This makes it much simpler to compile programs and run them at the same time.</p>

<p>Paging achieves that goal, and in addition:</p>

<ul>
  <li>
    <p>the switch between programs is very fast, because it is implemented by hardware</p>
  </li>
  <li>
    <p>the memory of both programs can grow and shrink as needed without too much fragmentation</p>
  </li>
  <li>
    <p>one program can never access the memory of another program, even if it wanted to.</p>

    <p>This is good both for security, and to prevent bugs in one program from crashing other programs.</p>
  </li>
</ul>

<h2 id="hardware-implementation">Hardware implementation</h2>

<p>Paging is implemented by the CPU hardware itself.</p>

<p>Paging could be implemented in software, but that would be too slow, because every single RAM memory access uses it!</p>

<p>Operating systems must setup and control paging by communicating to the CPU hardware. This is done mostly via:</p>

<ul>
  <li>
    <p>the CR3 register, which tells the CPU where the page table is in RAM memory</p>
  </li>
  <li>
    <p>writing the correct paging data structures to the RAM pointed to the CR3 register.</p>

    <p>Using RAM data structures is a common technique when lots of data must be transmitted to the CPU as it would cost too much to have such a large CPU register.</p>

    <p>The format of the configuration data structures is fixed <em>by the hardware</em>, but it is up to the OS to set up and manage those data structures on RAM correctly, and to tell the hardware where to find them (via <code class="highlighter-rouge">cr3</code>).</p>

    <p>Then some heavy caching is done to ensure that the RAM access will be fast, in particular using the TLB.</p>

    <p>Another notable example of RAM data structure used by the CPU is the <a href="https://en.wikipedia.org/wiki/Interrupt_descriptor_table">IDT</a> which sets up interrupt handlers.</p>
  </li>
</ul>

<p>The OS makes it impossible for programs to change the paging setup directly without going through the OS:</p>

<ul>
  <li>CR3 cannot be modified in ring 3. The OS runs in ring 0. See also:
    <ul>
      <li><a href="https://stackoverflow.com/questions/5957570/what-is-the-difference-between-the-kernel-space-and-the-user-space/44285809#44285809">https://stackoverflow.com/questions/5957570/what-is-the-difference-between-the-kernel-space-and-the-user-space/44285809#44285809</a></li>
      <li><a href="https://stackoverflow.com/questions/18717016/what-are-ring-0-and-ring-3-in-os">https://stackoverflow.com/questions/18717016/what-are-ring-0-and-ring-3-in-os</a></li>
    </ul>
  </li>
  <li>the page table structures are made invisible to the process using paging itself!</li>
</ul>

<p>Processes can however make requests to the OS that cause the page tables to be modified, notably:</p>

<ul>
  <li>stack size changes</li>
  <li><code class="highlighter-rouge">brk</code> and <code class="highlighter-rouge">mmap</code> calls, see also: <a href="https://stackoverflow.com/questions/6988487/what-does-brk-system-call-do/31082353#31082353">https://stackoverflow.com/questions/6988487/what-does-brk-system-call-do/31082353#31082353</a></li>
</ul>

<p>The kernel then decides if the request will be granted or not in a controlled manner.</p>

<h2 id="segmentation">Segmentation</h2>

<p>In x86 systems, there may actually be 2 address translation steps:</p>

<ul>
  <li>first segmentation</li>
  <li>then paging</li>
</ul>

<p>As such:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(logical) ------------------&gt; (linear) ------------&gt; (physical)
             segmentation                 paging
</code></pre></div></div>

<p>The major difference between paging and segmentation is that:</p>

<ul>
  <li>paging splits RAM into equal sized chunks called pages</li>
  <li>segmentation splits memory into chunks of arbitrary sizes</li>
</ul>

<p>This is the main advantage of paging, since equal sized chunks make things more manageable by reducing memory fragmentation problems. See also:</p>

<ul>
  <li><a href="https://stackoverflow.com/questions/16643180/differences-or-similarities-between-segmented-paging-and-paged-segmentation">https://stackoverflow.com/questions/16643180/differences-or-similarities-between-segmented-paging-and-paged-segmentation</a></li>
  <li><a href="https://softwareengineering.stackexchange.com/questions/100047/why-not-segmentation">https://softwareengineering.stackexchange.com/questions/100047/why-not-segmentation</a></li>
  <li><a href="https://www.quora.com/What-is-the-difference-between-paging-and-segment-in-memory-management">https://www.quora.com/What-is-the-difference-between-paging-and-segment-in-memory-management</a></li>
</ul>

<p>Paging came after segmentation historically, and largely replaced it for the implementation of virtual memory in modern OSs.</p>

<p>Paging has become so much more popular that support for segmentation was dropped in x86-64 in 64-bit mode, the main mode of operation for new software, where it only exists in compatibility mode, which emulates IA-32.</p>

<h2 id="example-simplified-single-level-paging-scheme">Example: simplified single-level paging scheme</h2>

<p>This is an example of how paging operates on a <em>simplified</em> version of a x86 architecture to implement a virtual memory space with a <code class="highlighter-rouge">20 | 12</code> address split (4 KiB page size).</p>

<h3 id="single-level-paging-scheme-visualization">Single level paging scheme visualization</h3>

<p>This is how the memory could look like in a single level paging scheme:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Links   Data                    Physical address

      +-----------------------+ 2^32 - 1
      |                       |
      .                       .
      |                       |
      +-----------------------+ page0 + 4k
      | data of page 0        |
+----&gt;+-----------------------+ page0
|     |                       |
|     .                       .
|     |                       |
|     +-----------------------+ pageN + 4k
|     | data of page N        |
|  +-&gt;+-----------------------+ pageN
|  |  |                       |
|  |  .                       .
|  |  |                       |
|  |  +-----------------------+ CR3 + 2^20 * 4
|  +--| entry[2^20-1] = pageN |
|     +-----------------------+ CR3 + 2^20 - 1 * 4
|     |                       |
|     .    many entires       .
|     |                       |
|     +-----------------------+ CR3 + 2 * 4
|  +--| entry[1] = page1      |
|  |  +-----------------------+ CR3 + 1 * 4
+-----| entry[0] = page0      |
   |  +-----------------------+ &lt;--- CR3
   |  |                       |
   |  .                       .
   |  |                       |
   |  +-----------------------+ page1 + 4k
   |  | data of page 1        |
   +-&gt;+-----------------------+ page1
      |                       |
      .                       .
      |                       |
      +-----------------------+  0
</code></pre></div></div>

<p>Notice that:</p>

<ul>
  <li>the CR3 register points to the first entry of the page table</li>
  <li>the page table is just a large array with 2^20 page table entries</li>
  <li>each entry is 4 bytes big, so the array takes up 4 MiB</li>
  <li>each page table contains the physical address a page</li>
  <li>each page is a 4 KiB aligned 4KiB chunk of memory that user processes may use</li>
  <li>we have 2^20 table entries. Since each page is 4KiB == 2^12, this covers the whole 4GiB (2^32) of 32-bit memory</li>
</ul>

<h3 id="single-level-paging-scheme-numerical-translation-example">Single level paging scheme numerical translation example</h3>

<p>Suppose that the OS has setup the following page tables for process 1:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>entry index   entry address       page address   present
-----------   ------------------  ------------   -------
0             CR3_1 + 0      * 4  0x00001        1
1             CR3_1 + 1      * 4  0x00000        1
2             CR3_1 + 2      * 4  0x00003        1
3             CR3_1 + 3      * 4                 0
...
2^20-1        CR3_1 + 2^20-1 * 4  0x00005        1
</code></pre></div></div>

<p>And for process 2:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>entry index   entry address       page address   present
-----------   -----------------   ------------   -------
0             CR3_2 + 0      * 4  0x0000A        1
1             CR3_2 + 1      * 4  0x12345        1
2             CR3_2 + 2      * 4                 0
3             CR3_2 + 3      * 4  0x00003        1
...
2^20-1        CR3_2 + 2^20-1 * 4  0xFFFFF        1
</code></pre></div></div>

<p>Before process 1 starts running, the OS sets its <code class="highlighter-rouge">cr3</code> to point to the page table 1 at <code class="highlighter-rouge">CR3_1</code>.</p>

<p>When process 1 tries to access a linear address, this is the physical addresses that will be actually accessed:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>linear     physical
---------  ---------
00000 001  00001 001
00000 002  00001 002
00000 003  00001 003
00000 FFF  00001 FFF
00001 000  00000 000
00001 001  00000 001
00001 FFF  00000 FFF
00002 000  00003 000
FFFFF 000  00005 000
</code></pre></div></div>

<p>To switch to process 2, the OS simply sets <code class="highlighter-rouge">cr3</code> to <code class="highlighter-rouge">CR3_2</code>, and now the following translations would happen:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>linear     physical
---------  ---------
00000 002  0000A 002
00000 003  0000A 003
00000 FFF  0000A FFF
00001 000  12345 000
00001 001  12345 001
00001 FFF  12345 FFF
00004 000  00003 000
FFFFF 000  FFFFF 000
</code></pre></div></div>

<p>Step-by-step translation for process 1 of logical address <code class="highlighter-rouge">0x00000001</code> to physical address <code class="highlighter-rouge">0x00001001</code>:</p>

<ul>
  <li>
    <p>split the linear address into two parts:</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>| page (20 bits) | offset (12 bits) |
</code></pre></div>    </div>

    <p>So in this case we would have:</p>

    <ul>
      <li>page = 0x00000. This part must be translated to a physical location.</li>
      <li>offset = 0x001. This part is added directly to the page address, and is not translated: it contains the position <em>within</em> the page.</li>
    </ul>
  </li>
  <li>
    <p>look into Page table 1 because <code class="highlighter-rouge">cr3</code> points to it.</p>
  </li>
  <li>
    <p>The hardware knows that this entry is located at RAM address <code class="highlighter-rouge">CR3 + 0x00000 * 4 = CR3</code>:</p>

    <ul>
      <li><code class="highlighter-rouge">0x00000</code> because the page part of the logical address is <code class="highlighter-rouge">0x00000</code></li>
      <li><code class="highlighter-rouge">4</code> because that is the fixed size in bytes of every page table entry</li>
    </ul>
  </li>
  <li>
    <p>since it is present, the access is valid</p>
  </li>
  <li>
    <p>by the page table, the location of page number <code class="highlighter-rouge">0x00000</code> is at <code class="highlighter-rouge">0x00001 * 4K = 0x00001000</code>.</p>
  </li>
  <li>
    <p>to find the final physical address we just need to add the offset:</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  00001 000
+ 00000 001
  ---------
  00001 001
</code></pre></div>    </div>

    <p>because <code class="highlighter-rouge">00001</code> is the physical address of the page looked up on the table and <code class="highlighter-rouge">001</code> is the offset.</p>

    <p>We shift <code class="highlighter-rouge">00001</code> by 12 bits because the pages are always aligned to 4KiB.</p>

    <p>The offset is always simply added the physical address of the page.</p>
  </li>
  <li>
    <p>the hardware then gets the memory at that physical location and puts it in a register.</p>
  </li>
</ul>

<p>Another example: for logical address <code class="highlighter-rouge">0x00001001</code>:</p>

<ul>
  <li>the page part is <code class="highlighter-rouge">00001</code>, and the offset part is <code class="highlighter-rouge">001</code></li>
  <li>the hardware knows that its page table entry is located at RAM address: <code class="highlighter-rouge">CR3 + 1 * 4</code> (<code class="highlighter-rouge">1</code> because of the page part), and that is where it will look for it</li>
  <li>it finds the page address <code class="highlighter-rouge">0x00000</code> there</li>
  <li>so the final address is <code class="highlighter-rouge">0x00000 * 4k + 0x001 = 0x00000001</code></li>
</ul>

<h3 id="multiple-addresses-translate-to-a-single-physical-address">Multiple addresses translate to a single physical address</h3>

<p>The same linear address can translate to different physical addresses for different processes, depending only on the value inside <code class="highlighter-rouge">cr3</code>.</p>

<p>Both linear addresses <code class="highlighter-rouge">00002 000</code> from process 1 and <code class="highlighter-rouge">00004 000</code> from process 2 point to the same physical address <code class="highlighter-rouge">00003 000</code>. This is completely allowed by the hardware, and it is up to the operating system to handle such cases.</p>

<p>This often in normal operation because of Copy-on-write (COW), which be explained elsewhere.</p>

<p>Such mappings are sometime called “aliases”.</p>

<h3 id="identity-mapping">Identity mapping</h3>

<p><code class="highlighter-rouge">FFFFF 000</code> points to its own physical address <code class="highlighter-rouge">FFFFF 000</code>. This kind of translation is called an “identity mapping”, and can be very convenient for OS-level debugging.</p>

<h3 id="page-faults">Page faults</h3>

<p>What if Process 1 tries to access <code class="highlighter-rouge">0x00003000</code>, which is not present?</p>

<p>The hardware notifies the software via a Page Fault Exception.</p>

<p>When an exception happens, the CPU jumps to an address that the OS had previously registered as the fault handler. This is usually done at boot time by the OS.</p>

<p>This could happen for example due to a programming error:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>int *is = malloc(1);
is[2] = 1;
</code></pre></div></div>

<p>but there are cases where it is not a bug, for example in Linux when:</p>

<ul>
  <li>
    <p>the program wants to increase its stack.</p>

    <p>It just tries to accesses a certain byte in a given possible range, and if the OS is happy it adds that page to the process address space, otherwise, it sends a signal to the process.</p>
  </li>
  <li>
    <p>the page was swapped to disk.</p>

    <p>The OS will need to do some work behind the processes back to get the page back into RAM.</p>

    <p>The OS can discover that this is the case based on the contents of the rest of the page table entry, since if the present flag is clear, the other entries of the page table entry are completely left for the OS to to what it wants.</p>

    <p>On Linux for example, when present = 0:</p>

    <ul>
      <li>
        <p>if all the fields of the page table entry are 0, invalid address.</p>
      </li>
      <li>
        <p>else, the page has been swapped to disk, and the actual values of those fields encode the position of the page on the disk.</p>
      </li>
    </ul>
  </li>
</ul>

<p>In any case, the OS needs to know which address generated the Page Fault to be able to deal with the problem. This is why the nice IA32 developers set the value of <code class="highlighter-rouge">cr2</code> to that address whenever a Page Fault occurs. The exception handler can then just look into <code class="highlighter-rouge">cr2</code> to get the address.</p>

<h3 id="page-table-entries">Page table entries</h3>

<p>The exact format of table entries is fixed <em>by the hardware</em>.</p>

<p>Each page entry can be seen as a <code class="highlighter-rouge">struct</code> with many fields.</p>

<p>The page table is then an array of <code class="highlighter-rouge">struct</code>.</p>

<p>On this simplified example, the page table entries contain only two fields:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bits   function
-----  -----------------------------------------
20     physical address of the start of the page
1      present flag
</code></pre></div></div>

<p>so in this example the hardware designers could have chosen the size of the page table to b <code class="highlighter-rouge">21</code> instead of <code class="highlighter-rouge">32</code> as we’ve used so far.</p>

<p>All real page table entries have other fields, notably fields to set pages to read-only for Copy-on-write. This will be explained elsewhere.</p>

<p>It would be impractical to align things at 21 bytes since memory is addressable by bytes and not bits. Therefore, even in only 21 bits are needed in this case, hardware designers would probably choose 32 to make access faster, and just reserve bits the remaining bits for later usage. The actual value on x86 is 32 bits.</p>

<p>Here is a screenshot from the Intel manual image “Formats of CR3 and Paging-Structure Entries with 32-Bit Paging” showing the structure of a page table in all its glory:</p>

<p><a href="/x86-page-entry.png"><img src="/x86-page-entry.png" alt="" style="width:700px;" /></a></p>

<p>The fields are explained in the manual just after.</p>

<h3 id="page-size-choice">Page size choice</h3>

<p>Why are pages 4KiB anyways?</p>

<p>There is a trade-off between memory wasted in:</p>

<ul>
  <li>page tables</li>
  <li>extra padding memory within pages</li>
</ul>

<p>This can be seen with the extreme cases:</p>

<ul>
  <li>if the page size were 1 byte:
    <ul>
      <li>granularity would be great, and the OS would never have to allocate unneeded padding memory</li>
      <li>but the page table would have 2^32 entries, and take up the entire memory!</li>
    </ul>
  </li>
  <li>if the page size were 4GiB:
    <ul>
      <li>we would need to swap 4GiB to disk every time a new process becomes active</li>
      <li>the page size would be a single entry, so it would take almost no memory at all</li>
    </ul>
  </li>
</ul>

<p>x86 designers have found that 4KiB pages are a good middle ground.</p>

<h2 id="example-multi-level-paging-scheme">Example: multi-level paging scheme</h2>

<h3 id="the-problem-with-single-level-paging">The problem with single-level paging</h3>

<p>The problem with a single-level paging scheme is that it would take up too much RAM: 4G / 4K = 1M entries <em>per</em> process.</p>

<p>If each entry is 4 bytes long, that would make 4M <em>per process</em>, which is too much even for a desktop computer: <code class="highlighter-rouge">ps -A | wc -l</code> says that I am running 244 processes right now, so that would take around 1GB of my RAM!</p>

<p>For this reason, x86 developers decided to use a multi-level scheme that reduces RAM usage.</p>

<p>The downside of this system is that is has a slightly higher access time, as we need to access RAM more times for each translation.</p>

<h3 id="k-ary-trees-to-the-rescue">K-ary trees to the rescue</h3>

<p>The algorithmically minded will have noticed that paging requires <a href="https://en.wikipedia.org/wiki/Associative_array">associative array</a> (like Java <code class="highlighter-rouge">Map</code> of Python <code class="highlighter-rouge">dict()</code>) abstract data structure where:</p>

<ul>
  <li>the keys are linear pages addresses, thus of integer type</li>
  <li>the values are physical page addresses, also of integer type</li>
</ul>

<p>The single level paging scheme uses a simple array implementation of the associative array:</p>

<ul>
  <li>the keys are the array index</li>
  <li>this implementation is very fast in time</li>
  <li>but it is too inefficient in memory</li>
</ul>

<p>and in C pseudo-code it looks like this:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>linear_address[0]      = physical_address_0
linear_address[1]      = physical_address_1
linear_address[2]      = physical_address_2
...
linear_address[2^20-1] = physical_address_N
</code></pre></div></div>

<p>But there another simple associative array implementation that overcomes the memory problem: an (unbalanced) <a href="https://en.wikipedia.org/wiki/K-ary_tree">K-ary tree</a>.</p>

<p>A K-ary tree, is just like a <a href="https://en.wikipedia.org/wiki/Binary_tree">binary tree</a>, but with K children instead of 2.</p>

<p>Using a K-ary tree instead of an array implementation has the following trade-offs:</p>

<ul>
  <li>it uses way less memory</li>
  <li>it is slower since we have to de-reference extra pointers</li>
</ul>

<p>In C-pseudo code, a 2-level K-ary tree with <code class="highlighter-rouge">K = 2^10</code> looks like this:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>level0[0] = &amp;level1_0[0]
    level1_0[0]      = physical_address_0_0
    level1_0[1]      = physical_address_0_1
    ...
    level1_0[2^10-1] = physical_address_0_N
level0[1] = &amp;level1_1[0]
    level1_1[0]      = physical_address_1_0
    level1_1[1]      = physical_address_1_1
    ...
    level1_1[2^10-1] = physical_address_1_N
...
level0[N] = &amp;level1_N[0]
    level1_N[0]      = physical_address_N_0
    level1_N[1]      = physical_address_N_1
    ...
    level1_N[2^10-1] = physical_address_N_N
</code></pre></div></div>

<p>and we have the following arrays:</p>

<ul>
  <li>one <code class="highlighter-rouge">directory</code>, which has <code class="highlighter-rouge">2^10</code> elements. Each element contains a pointer to a page table array.</li>
  <li>up to 2^10 <code class="highlighter-rouge">pagetable</code> arrays. Each one has <code class="highlighter-rouge">2^10</code> 4 byte page entries.</li>
</ul>

<p>and it still contains <code class="highlighter-rouge">2^10 * 2^10 = 2^20</code> possible keys.</p>

<p>K-ary trees can save up a lot of space, because if we only have one key, then we only need the following arrays:</p>

<ul>
  <li>one <code class="highlighter-rouge">directory</code> with 2^10 entries</li>
  <li>one <code class="highlighter-rouge">pagetable</code> at <code class="highlighter-rouge">directory[0]</code> with 2^10 entries</li>
  <li>all other <code class="highlighter-rouge">directory[i]</code> are marked as invalid, don’t point to anything, and we don’t allocate <code class="highlighter-rouge">pagetable</code> for them at all</li>
</ul>

<h3 id="why-not-a-balanced-tree">Why not a balanced tree</h3>

<p>Learned readers will ask themselves: so why use an unbalanced tree instead of balanced one, which offers better asymptotic times <a href="https://en.wikipedia.org/wiki/Self-balancing_binary_search_tree">https://en.wikipedia.org/wiki/Self-balancing_binary_search_tree</a>?</p>

<p>Likely:</p>

<ul>
  <li>the maximum number of entries is small enough due to memory size limitations, that we won’t waste too much memory with the root directory entry</li>
  <li>different entries would have different levels, and thus different access times</li>
  <li>tree rotations would likely make caching more complicated</li>
</ul>

<h3 id="how-the-k-ary-tree-is-used-in-x86">How the K-ary tree is used in x86</h3>

<p>x86’s multi-level paging scheme uses a 2 level K-ary tree with 2^10 bits on each level.</p>

<p>Addresses are now split as:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>| directory (10 bits) | table (10 bits) | offset (12 bits) |
</code></pre></div></div>

<p>Then:</p>

<ul>
  <li>
    <p>the top 10 bits are used to walk the top level of the K-ary tree (<code class="highlighter-rouge">level0</code>)</p>

    <p>The top table is called a “directory of page tables”.</p>

    <p><code class="highlighter-rouge">cr3</code> now points to the location on RAM of the page directory of the current process instead of page tables.</p>

    <p>Page directory entries are very similar to page table entries except that they point to the physical addresses of page tables instead of physical addresses of pages.</p>

    <p>Each directory entry also takes up 4 bytes, just like page entries, so that makes 4 KiB per process minimum.</p>

    <p>Page directory entries also contain a valid flag: if invalid, the OS does not allocate a page table for that entry, and saves memory.</p>

    <p>Each process has one and only one page directory associated to it (and pointed to by <code class="highlighter-rouge">cr3</code>), so it will contain at least <code class="highlighter-rouge">2^10 = 1K</code> page directory entries, much better than the minimum 1M entries required on a single-level scheme.</p>
  </li>
  <li>
    <p>the next 10 bits are used to walk the second level of the K-ary tree (<code class="highlighter-rouge">level1</code>)</p>

    <p>Second level entries are also called page tables like the single level scheme.</p>

    <p>Page tables are only allocated only as needed by the OS.</p>

    <p>Each page table has only <code class="highlighter-rouge">2^10 = 1K</code> page table entries instead of <code class="highlighter-rouge">2^20</code> for the single paging scheme.</p>

    <p>Each process can now have up to <code class="highlighter-rouge">2^10</code> page tables instead of <code class="highlighter-rouge">2^20</code> for the single paging scheme.</p>
  </li>
  <li>
    <p>the offset is again not used for translation, it only gives the offset within a page</p>
  </li>
</ul>

<p>One reason for using 10 bits on the first two levels (and not, say, <code class="highlighter-rouge">12 | 8 | 12</code> ) is that each Page Table entry is 4 bytes long. Then the 2^10 entries of Page directories and Page Tables will fit nicely into 4Kb pages. This means that it faster and simpler to allocate and deallocate pages for that purpose.</p>

<h3 id="multi-level-paging-scheme-numerical-translation-example">Multi-level paging scheme numerical translation example</h3>

<p>Page directory given to process by the OS:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>entry index   entry address      page table address  present
-----------   ----------------   ------------------  --------
0             CR3 + 0      * 4   0x10000             1
1             CR3 + 1      * 4                       0
2             CR3 + 2      * 4   0x80000             1
3             CR3 + 3      * 4                       0
...
2^10-1        CR3 + 2^10-1 * 4                       0
</code></pre></div></div>

<p>Page tables given to process by the OS at <code class="highlighter-rouge">PT1 = 0x10000000</code> (<code class="highlighter-rouge">0x10000</code> * 4K):</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>entry index   entry address      page address  present
-----------   ----------------   ------------  -------
0             PT1 + 0      * 4   0x00001       1
1             PT1 + 1      * 4                 0
2             PT1 + 2      * 4   0x0000D       1
...                                  ...
2^10-1        PT1 + 2^10-1 * 4   0x00005       1
</code></pre></div></div>

<p>Page tables given to process by the OS at <code class="highlighter-rouge">PT2  = 0x80000000</code> (<code class="highlighter-rouge">0x80000</code> * 4K):</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>entry index   entry address     page address  present
-----------   ---------------   ------------  ------------
0             PT2 + 0     * 4   0x0000A       1
1             PT2 + 1     * 4   0x0000C       1
2             PT2 + 2     * 4                 0
...
2^10-1        PT2 + 0x3FF * 4   0x00003       1
</code></pre></div></div>

<p>where <code class="highlighter-rouge">PT1</code> and <code class="highlighter-rouge">PT2</code>: initial position of page table 1 and page table 2 for process 1 on RAM.</p>

<p>With that setup, the following translations would happen:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>linear    10 10 12 split  physical
--------  --------------  ----------
00000001  000 000 001     00001001
00001001  000 001 001     page fault
003FF001  000 3FF 001     00005001
00400000  001 000 000     page fault
00800001  002 000 001     0000A001
00801004  002 001 004     0000C004
00802004  002 002 004     page fault
00B00001  003 000 000     page fault
</code></pre></div></div>

<p>Let’s translate the linear address <code class="highlighter-rouge">0x00801004</code> step by step:</p>

<ul>
  <li>
    <p>In binary the linear address is:</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0    0    8    0    1    0    0    4
0000 0000 1000 0000 0001 0000 0000 0100
</code></pre></div>    </div>
  </li>
  <li>
    <p>Grouping as <code class="highlighter-rouge">10 | 10 | 12</code> gives:</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0000000010 0000000001 000000000100
0x2        0x1        0x4
</code></pre></div>    </div>

    <p>which gives:</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>page directory entry = 0x2
page table     entry = 0x1
offset               = 0x4
</code></pre></div>    </div>

    <p>So the hardware looks for entry 2 of the page directory.</p>
  </li>
  <li>
    <p>The page directory table says that the page table is located at <code class="highlighter-rouge">0x80000 * 4K = 0x80000000</code>. This is the first RAM access of the process.</p>

    <p>Since the page table entry is <code class="highlighter-rouge">0x1</code>, the hardware looks at entry 1 of the page table at <code class="highlighter-rouge">0x80000000</code>, which tells it that the physical page is located at address <code class="highlighter-rouge">0x0000C * 4K = 0x0000C000</code>. This is the second RAM access of the process.</p>
  </li>
  <li>
    <p>Finally, the paging hardware adds the offset, and the final address is <code class="highlighter-rouge">0x0000C004</code>.</p>
  </li>
</ul>

<p>Page faults occur if either a page directory entry or a page table entry is not present.</p>

<p>The Intel manual gives a picture of this translation process in the image “Linear-Address Translation to a 4-KByte Page using 32-Bit Paging”:</p>

<p><a href="/x86-page-translation.png"><img src="/x86-page-translation.png" alt="" /></a></p>

<h2 id="64-bit-architectures">64-bit architectures</h2>

<p>64 bits is still too much address for current RAM sizes, so most architectures will use less bits.</p>

<p>x86_64 uses 48 bits (256 TiB), and legacy mode’s PAE already allows 52-bit addresses (4 PiB). 56-bits is a likely future candidate.</p>

<p>12 of those 48 bits are already reserved for the offset, which leaves 36 bits.</p>

<p>If a 2 level approach is taken, the best split would be two 18 bit levels.</p>

<p>But that would mean that the page directory would have <code class="highlighter-rouge">2^18 = 256K</code> entries, which would take too much RAM: close to a single-level paging for 32 bit architectures!</p>

<p>Therefore, 64 bit architectures create even further page levels, commonly 3 or 4.</p>

<p>x86_64 uses 4 levels in a <code class="highlighter-rouge">9 | 9 | 9 | 9</code> scheme, so that the upper level only takes up only <code class="highlighter-rouge">2^9</code> higher level entries.</p>

<p>The 48 bits are split equally into two disjoint parts:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>----------------- FFFFFFFF FFFFFFFF
Top half
----------------- FFFF8000 00000000


Not addressable


----------------- 00007FFF FFFFFFFF
Bottom half
----------------- 00000000 00000000
</code></pre></div></div>

<p>A 5-level scheme is emerging in 2016: <a href="https://software.intel.com/sites/default/files/managed/2b/80/5-level_paging_white_paper.pdf">https://software.intel.com/sites/default/files/managed/2b/80/5-level_paging_white_paper.pdf</a> which allows 52-bit addresses with 4k pagetables.</p>

<h2 id="pae">PAE</h2>

<p>Physical address extension.</p>

<p>With 32 bits, only 4GB RAM can be addressed.</p>

<p>This started becoming a limitation for large servers, so Intel introduced the PAE mechanism to Pentium Pro.</p>

<p>To relieve the problem, Intel added 4 new address lines, so that 64GB could be addressed.</p>

<p>Page table structure is also altered if PAE is on. The exact way in which it is altered depends on weather PSE is on or off.</p>

<p>PAE is turned on and off via the <code class="highlighter-rouge">PAE</code> bit of <code class="highlighter-rouge">cr4</code>.</p>

<p>Even if the total addressable memory is 64GB, individual process are still only able to use up to 4GB. The OS can however put different processes on different 4GB chunks.</p>

<h2 id="pse">PSE</h2>

<p>Page size extension.</p>

<p>Allows for pages to be 4M ( or 2M if PAE is on ) in length instead of 4K.</p>

<p>PSE is turned on and off via the <code class="highlighter-rouge">PAE</code> bit of <code class="highlighter-rouge">cr4</code>.</p>

<h2 id="pae-and-pse-page-table-schemes">PAE and PSE page table schemes</h2>

<p>If either PAE and PSE are active, different paging level schemes are used:</p>

<ul>
  <li>
    <p>no PAE and no PSE: <code class="highlighter-rouge">10 | 10 | 12</code></p>
  </li>
  <li>
    <p>no PAE and PSE: <code class="highlighter-rouge">10 | 22</code>.</p>

    <p>22 is the offset within the 4Mb page, since 22 bits address 4Mb.</p>
  </li>
  <li>
    <p>PAE and no PSE: <code class="highlighter-rouge">2 | 9 | 9 | 12</code></p>

    <p>The design reason why 9 is used twice instead of 10 is that now entries cannot fit anymore into 32 bits, which were all filled up by 20 address bits and 12 meaningful or reserved flag bits.</p>

    <p>The reason is that 20 bits are not enough anymore to represent the address of page tables: 24 bits are now needed because of the 4 extra wires added to the processor.</p>

    <p>Therefore, the designers decided to increase entry size to 64 bits, and to make them fit into a single page table it is necessary reduce the number of entries to 2^9 instead of 2^10.</p>

    <p>The starting 2 is a new Page level called Page Directory Pointer Table (PDPT), since it <em>points</em> to page directories and fill in the 32 bit linear address. PDPTs are also 64 bits wide.</p>

    <p><code class="highlighter-rouge">cr3</code> now points to PDPTs which must be on the fist four 4GB of memory and aligned on 32 bit multiples for addressing efficiency. This means that now <code class="highlighter-rouge">cr3</code> has 27 significative bits instead of 20: 2^5 for the 32 multiples * 2^27 to complete the 2^32 of the first 4GB.</p>
  </li>
  <li>
    <p>PAE and PSE: <code class="highlighter-rouge">2 | 9 | 21</code></p>

    <p>Designers decided to keep a 9 bit wide field to make it fit into a single page.</p>

    <p>This leaves 23 bits. Leaving 2 for the PDPT to keep things uniform with the PAE case without PSE leaves 21 for offset, meaning that pages are 2M wide instead of 4M.</p>
  </li>
</ul>

<h2 id="tlb">TLB</h2>

<p>The Translation Lookahead Buffer (TLB) is a cache for paging addresses.</p>

<p>Since it is a cache, it shares many of the design issues of the CPU cache, such as associativity level.</p>

<p>This section shall describe a simplified fully associative TLB with 4 single address entries. Note that like other caches, real TLBs are not usually fully associative.</p>

<h3 id="basic-operation">Basic operation</h3>

<p>After a translation between linear and physical address happens, it is stored on the TLB. For example, a 4 entry TLB starts in the following state:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  valid  linear  physical
  -----  ------  --------
&gt; 0      00000   00000
  0      00000   00000
  0      00000   00000
  0      00000   00000
</code></pre></div></div>

<p>The <code class="highlighter-rouge">&gt;</code> indicates the current entry to be replaced.</p>

<p>and after a page linear address <code class="highlighter-rouge">00003</code> is translated to a physical address <code class="highlighter-rouge">00005</code>, the TLB becomes:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  valid  linear  physical
  -----  ------  --------
  1      00003   00005
&gt; 0      00000   00000
  0      00000   00000
  0      00000   00000
</code></pre></div></div>

<p>and after a second translation of <code class="highlighter-rouge">00007</code> to <code class="highlighter-rouge">00009</code> it becomes:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  valid  linear  physical
  -----  ------  --------
  1      00003   00005
  1      00007   00009
&gt; 0      00000   00000
  0      00000   00000
</code></pre></div></div>

<p>Now if <code class="highlighter-rouge">00003</code> needs to be translated again, hardware first looks up the TLB and finds out its address with a single RAM access <code class="highlighter-rouge">00003 --&gt; 00005</code>.</p>

<p>Of course, <code class="highlighter-rouge">00000</code> is not on the TLB since no valid entry contains <code class="highlighter-rouge">00000</code> as a key.</p>

<h3 id="replacement-policy">Replacement policy</h3>

<p>When TLB is filled up, older addresses are overwritten. Just like CPU cache, the replacement policy is a potentially complex operation, but a simple and reasonable heuristic is to remove the least recently used entry (LRU).</p>

<p>With LRU, starting from state:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  valid  linear  physical
  -----  ------  --------
&gt; 1      00003   00005
  1      00007   00009
  1      00009   00001
  1      0000B   00003
</code></pre></div></div>

<p>adding <code class="highlighter-rouge">0000D -&gt; 0000A</code> would give:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  valid  linear  physical
  -----  ------  --------
  1      0000D   0000A
&gt; 1      00007   00009
  1      00009   00001
  1      0000B   00003
</code></pre></div></div>

<h3 id="cam">CAM</h3>

<p>Using the TLB makes translation faster, because the initial translation takes one access <em>per TLB level</em>, which means 2 on a simple 32 bit scheme, but 3 or 4 on 64 bit architectures.</p>

<p>The TLB is usually implemented as an expensive type of RAM called content-addressable memory (CAM). CAM implements an associative map on hardware, that is, a structure that given a key (linear address), retrieves a value.</p>

<p>Mappings could also be implemented on RAM addresses, but CAM mappings may required much less entries than a RAM mapping.</p>

<p>For example, a map in which:</p>

<ul>
  <li>both keys and values have 20 bits (the case of a simple paging schemes)</li>
  <li>at most 4 values need to be stored at each time</li>
</ul>

<p>could be stored in a TLB with 4 entries:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>linear  physical
------  --------
00000   00001
00001   00010
00010   00011
FFFFF   00000
</code></pre></div></div>

<p>However, to implement this with RAM, <em>it would be necessary to have 2^20 addresses</em>:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>linear  physical
------  --------
00000   00001
00001   00010
00010   00011
... (from 00011 to FFFFE)
FFFFF   00000
</code></pre></div></div>

<p>which would be even more expensive than using a TLB.</p>

<h3 id="invalidating-entries">Invalidating entries</h3>

<p>When <code class="highlighter-rouge">cr3</code> changes, all TLB entries are invalidated, because a new page table for a new process is going to be used, so it is unlikely that any of the old entries have any meaning.</p>

<p>The x86 also offers the <code class="highlighter-rouge">invlpg</code> instruction which explicitly invalidates a single TLB entry. Other architectures offer even more instructions to invalidated TLB entries, such as invalidating all entries on a given range.</p>

<h2 id="linux-kernel-usage">Linux kernel usage</h2>

<p>The Linux kernel makes extensive usage of the paging features of x86 to allow fast process switches with small data fragmentation.</p>

<h3 id="play-with-physical-addresses-in-linux">Play with physical addresses in Linux</h3>

<p>Convert virtual addresses to physical from user space with <code class="highlighter-rouge">/proc/&lt;pid&gt;/pagemap</code> and from kernel space with <code class="highlighter-rouge">virt_to_phys</code>:</p>

<ul>
  <li><a href="https://stackoverflow.com/questions/5748492/is-there-any-api-for-determining-the-physical-address-from-virtual-address-in-li/45128487#45128487">https://stackoverflow.com/questions/5748492/is-there-any-api-for-determining-the-physical-address-from-virtual-address-in-li/45128487#45128487</a></li>
  <li><a href="https://github.com/cirosantilli/linux-kernel-module-cheat/blob/1f4f7faebacca75267cc1d63bfeffc30080d017d/kernel_module/user/virt_to_phys_user.c">https://github.com/cirosantilli/linux-kernel-module-cheat/blob/1f4f7faebacca75267cc1d63bfeffc30080d017d/kernel_module/user/virt_to_phys_user.c</a></li>
  <li><code class="highlighter-rouge">virt_to_phys</code>:
    <ul>
      <li><a href="https://github.com/cirosantilli/linux-kernel-module-cheat/blob/0677dbd4b582d1a913462d75caad0abf21e87f32/kernel_module/virt_to_phys.c">https://github.com/cirosantilli/linux-kernel-module-cheat/blob/0677dbd4b582d1a913462d75caad0abf21e87f32/kernel_module/virt_to_phys.c</a></li>
      <li><a href="https://github.com/cirosantilli/linux-kernel-module-cheat/blob/1f4f7faebacca75267cc1d63bfeffc30080d017d/kernel_module/user/virt_to_phys_user.c">https://github.com/cirosantilli/linux-kernel-module-cheat/blob/1f4f7faebacca75267cc1d63bfeffc30080d017d/kernel_module/user/virt_to_phys_user.c</a></li>
    </ul>
  </li>
</ul>

<p>Dump all page tables from userspace with <code class="highlighter-rouge">/proc/&lt;pid&gt;/maps</code> and <code class="highlighter-rouge">/proc/&lt;pid&gt;/pagemap</code>:</p>

<ul>
  <li><a href="https://github.com/cirosantilli/linux-kernel-module-cheat/blob/1f4f7faebacca75267cc1d63bfeffc30080d017d/kernel_module/user/virt_to_phys_user.c">https://github.com/cirosantilli/linux-kernel-module-cheat/blob/1f4f7faebacca75267cc1d63bfeffc30080d017d/kernel_module/user/virt_to_phys_user.c</a></li>
  <li><a href="https://stackoverflow.com/questions/6284810/proc-pid-pagemaps-and-proc-pid-maps-linux/45500208#45500208">https://stackoverflow.com/questions/6284810/proc-pid-pagemaps-and-proc-pid-maps-linux/45500208#45500208</a></li>
</ul>

<p>Read and write physical addresses from userspace with <code class="highlighter-rouge">/dev/mem</code>:</p>

<ul>
  <li><a href="https://stackoverflow.com/questions/12040303/accessing-physical-address-from-user-space/45127890#45127890">https://stackoverflow.com/questions/12040303/accessing-physical-address-from-user-space/45127890#45127890</a></li>
  <li><a href="https://free-electrons.com/pub/mirror/devmem2.c">https://free-electrons.com/pub/mirror/devmem2.c</a></li>
</ul>

<h3 id="kernel-vs-process-memory-layout">Kernel vs process memory layout</h3>

<p>The Linux Kernel reserves two zones of virtual memory:</p>

<ul>
  <li>one for kernel memory</li>
  <li>one for programs</li>
</ul>

<p>The exact split is configured by <code class="highlighter-rouge">CONFIG_VMSPLIT_...</code>. By default:</p>

<ul>
  <li>
    <p>on 32-bit:</p>

    <ul>
      <li>the bottom 3/4 is program space: <code class="highlighter-rouge">00000000</code> to <code class="highlighter-rouge">BFFFFFFF</code></li>
      <li>the top 1/4 is kernel memory: <code class="highlighter-rouge">C0000000</code> to <code class="highlighter-rouge">FFFFFFFF</code></li>
    </ul>

    <p>Like this:</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>------------------ FFFFFFFF
Kernel
------------------ C0000000
------------------ BFFFFFFF


Process


------------------ 00000000
</code></pre></div>    </div>
  </li>
  <li>
    <p>on 64-bit: currently only 48-bits are actually used, split into two equally sized disjoint spaces. The Linux kernel just assigns:</p>

    <ul>
      <li>the bottom part to processes <code class="highlighter-rouge">00000000 00000000</code> to <code class="highlighter-rouge">008FFFFF FFFFFFFF</code></li>
      <li>the top part to the kernel: <code class="highlighter-rouge">FFFF8000 00000000</code> to <code class="highlighter-rouge">FFFFFFFF FFFFFFFF</code></li>
    </ul>

    <p>Like this:</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>------------------ FFFFFFFF
Kernel
------------------ C0000000


(not addressable)


------------------ BFFFFFFF
Process
------------------ 00000000
</code></pre></div>    </div>
  </li>
</ul>

<p>Kernel memory <a href="https://stackoverflow.com/questions/18953598/is-it-true-that-whole-system-space-address-space-in-linux-does-not-use-demand-pa">is also paged</a>.</p>

<p>In previous versions, <a href="https://stackoverflow.com/questions/1658757/linux-3-1-virtual-address-split">the paging was continuous, but with HIGHMEM this changed</a>.</p>

<p>There is no clear physical memory split: <a href="https://stackoverflow.com/questions/30471742/physical-memory-userspace-kernel-split-on-linux-x86-64">https://stackoverflow.com/questions/30471742/physical-memory-userspace-kernel-split-on-linux-x86-64</a></p>

<h3 id="process-memory-layout">Process memory layout</h3>

<p>For each process, the virtual address space looks like this:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>------------------ 2^32 - 1
Stack (grows down)
v v v v v v v v v
------------------

(unmapped)

------------------ Maximum stack size.


(unmapped)


-------------------
mmap
-------------------


(unmapped)


-------------------
^^^^^^^^^^^^^^^^^^^
brk (grows up)
-------------------
BSS
-------------------
Data
-------------------
Text
-------------------

------------------- 0
</code></pre></div></div>

<p>The kernel maintains a list of pages that belong to each process, and synchronizes that with the paging.</p>

<p>If the program accesses memory that does not belong to it, the kernel handles a page-fault, and decides what to do:</p>

<ul>
  <li>if it is above the maximum stack size, allocate those pages to the process</li>
  <li>otherwise, send a SIGSEGV to the process, which usually kills it</li>
</ul>

<p>When an ELF file is loaded by the kernel to start a program with the <code class="highlighter-rouge">exec</code> system call, the kernel automatically registers text, data, BSS and stack for the program.</p>

<p>The <code class="highlighter-rouge">brk</code> and <code class="highlighter-rouge">mmap</code> areas can be modified by request of the program through the <a href="https://stackoverflow.com/questions/6988487/what-does-brk-system-call-do/31082353#31082353"><code class="highlighter-rouge">brk</code></a> and <code class="highlighter-rouge">mmap</code> system calls. But the kernel can also deny the program those areas if there is not enough memory.</p>

<p><code class="highlighter-rouge">brk</code> and <code class="highlighter-rouge">mmap</code> can be used to implement <code class="highlighter-rouge">malloc</code>, or the so called “heap”.</p>

<p><code class="highlighter-rouge">mmap</code> is also used to load dynamically loaded libraries into the program’s memory so that it can access and run it.</p>

<p>Stack allocation: <a href="https://stackoverflow.com/questions/17671423/stack-allocation-for-process">https://stackoverflow.com/questions/17671423/stack-allocation-for-process</a></p>

<p>Calculating exact addresses Things are complicated by:</p>

<ul>
  <li><a href="https://en.wikipedia.org/wiki/Address_space_layout_randomization">Address Space Layout Randomization</a>.</li>
  <li>the fact that environment variables, CLI arguments, and some ELF header data take up initial stack space: <a href="https://unix.stackexchange.com/questions/145557/how-does-stack-allocation-work-in-linux/239323#239323">https://unix.stackexchange.com/questions/145557/how-does-stack-allocation-work-in-linux/239323#239323</a></li>
</ul>

<p>Why the text does not start at 0: <a href="https://stackoverflow.com/questions/14795164/why-do-linux-program-text-sections-start-at-0x0804800-and-stack-tops-start-at-0">https://stackoverflow.com/questions/14795164/why-do-linux-program-text-sections-start-at-0x0804800-and-stack-tops-start-at-0</a></p>

<h3 id="copy-on-write">Copy-on-write</h3>

<p><a href="https://en.wikipedia.org/wiki/Copy-on-write">https://en.wikipedia.org/wiki/Copy-on-write</a></p>

<p>Besides a missing page, a very common source of page faults is copy-on-write (COW).</p>

<p>Page tables have extra flags that allow the OS to mark a page a read-only.</p>

<p>Those page faults only happen when a process tries to write to the page, and not read from it.</p>

<p>When Linux forks a process:</p>

<ul>
  <li>instead of copying all the pages, which is unnecessarily costly, it makes the page tables of the two process point to the same physical address.</li>
  <li>it marks those linear addresses as read-only</li>
  <li>whenever one of the processes tries to write to a page, the makes a copy of the physical memory, and updates the pages of the two process to point to the two different physical addresses</li>
</ul>

<h3 id="linux-source-tree">Linux source tree</h3>

<p>In <code class="highlighter-rouge">v4.2</code>, look under <code class="highlighter-rouge">arch/x86/</code>:</p>

<ul>
  <li><code class="highlighter-rouge">include/asm/pgtable*</code></li>
  <li><code class="highlighter-rouge">include/asm/page*</code></li>
  <li><code class="highlighter-rouge">mm/pgtable*</code></li>
  <li><code class="highlighter-rouge">mm/page*</code></li>
</ul>

<p>There seems to be no structs defined to represent the pages, only macros: <code class="highlighter-rouge">include/asm/page_types.h</code> is specially interesting. Excerpt:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#define _PAGE_BIT_PRESENT   0   /* is present */
#define _PAGE_BIT_RW        1   /* writeable */
#define _PAGE_BIT_USER      2   /* userspace addressable */
#define _PAGE_BIT_PWT       3   /* page write through */
</code></pre></div></div>

<p><code class="highlighter-rouge">arch/x86/include/uapi/asm/processor-flags.h</code> defines <code class="highlighter-rouge">CR0</code>, and in particular the <code class="highlighter-rouge">PG</code> bit position:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#define X86_CR0_PG_BIT      31 /* Paging */
</code></pre></div></div>

<h2 id="memory-management-unit">Memory management unit</h2>

<p>Paging is done by the <a href="https://en.wikipedia.org/wiki/Memory_management_unit">Memory Management Unit</a> (MMU) part of the CPU.</p>

<p>Like many others (e.g. <a href="https://en.wikipedia.org/wiki/X87">x87 co-processor</a>, <a href="https://en.wikipedia.org/wiki/Advanced_Programmable_Interrupt_Controller">APIC</a>), this used to be by separate chip on early days.</p>

<p>It was later integrated into the CPU, but the term MMU still used.</p>

<h2 id="second-level-address-translation">Second Level Address Translation</h2>

<p><a href="https://en.wikipedia.org/wiki/Second_Level_Address_Translation">https://en.wikipedia.org/wiki/Second_Level_Address_Translation</a></p>

<p>Two level address translation to make OS emulation more efficient.</p>

<h2 id="other-architectures">Other architectures</h2>

<p><a href="https://stackoverflow.com/a/32258855/895245">Peter Cordes mentions</a> that some architectures like MIPS leave paging almost completely in the hands of software: a TLB miss runs an OS-supplied function to walk the page tables, and insert the new mapping into the TLB. In such architectures, the OS can use whatever data structure it wants.</p>

<h3 id="arm">ARM</h3>

<p>Some interesting features and characteristics of ARMv8 page tables:</p>

<ul>
  <li>9 9 9 9 address split</li>
  <li>pages other than the last level (3 if you count from 0) can be leaves. When this happens, the page is larger than 4k (since more bits are left for the offset)</li>
</ul>

<h2 id="bibliography">Bibliography</h2>

<p>Free:</p>

<ul>
  <li>
    <p><a href="https://www.cs.rutgers.edu/~pxk/416/notes/">rutgers-pxk-416</a> chapter “Memory management: lecture notes”</p>

    <p>Good historical review of memory organization techniques used by older OS.</p>
  </li>
</ul>

<p>Non-free:</p>

<ul>
  <li>
    <p><a href="https://www.amazon.com/books/dp/0596005652">bovet05</a> chapter “Memory addressing”</p>

    <p>Reasonable intro to x86 memory addressing. Missing some good and simple examples.</p>
  </li>
</ul>


        </main>
      </div>
      <footer>
        <p>Content licence: <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> unless otherwise noted on the article.</p>
        <p>Website powered by <a href="https://github.com/jekyll/jekyll">Jekyll</a> and <a href="https://pages.github.com/">GitHub Pages</a>. Source at: <a href="https://github.com/cirosantilli/cirosantilli.github.io">https://github.com/cirosantilli/cirosantilli.github.io</a></p>
        <p>To contact me publicly about any general subject, including saying hi or suggestions about this website, please <a href="https://github.com/cirosantilli/cirosantilli.github.io/issues/new">create a GitHub issue in the cirosantilli.github.io repo</a>.</p>
        <p>
          For comments about China, please first read
          <a href="https://github.com/cirosantilli/china-dictatorship#faq">the FAQ</a> and
          <a href="https://github.com/cirosantilli/china-dictatorship/blob/master/CONTRIBUTING.md">the CONTRIBUTING</a> and then
          <a href="https://github.com/cirosantilli/china-dictatorship/issues/new">create a GitHub issue in the china-dictatorship repo</a>.
        </p>
        <p>If you absolutely need private contact, extract my email from my GitHub repos or use LinkedIn.</p>
        <p>
          <a href="https://en.wikipedia.org/wiki/Disqus">Disqus comments</a> were removed from this website in 2019-05-04,
          a manual dump is <a href="/disqus-archive/">available here</a>.
          <a href="https://github.com/cirosantilli/cirosantilli.github.io/tree/fee98be02a34d5d4ccf829c4df89f1a5140881fc#why-i-removed-disqus-comments-from-the-website-in-2019-05-04">Rationale</a>.
        </p>
        <p>Opinions and content are my own, not my employer's.</p>
      </footer>
        
    </div>
    <!-- Scripts -->
      <!--data table-->
        <script src="https://ajax.aspnetcdn.com/ajax/jquery.dataTables/1.9.4/jquery.dataTables.min.js" ></script>
        <script>
          $('.data-table').dataTable({
            //"aaSorting": [[0,'asc'], [1,'asc'], [2, 'asc'], [3, 'asc']],
            "aaSorting": [],
            "bFilter": false,
            "bInfo": false,
            "bPaginate": false
          });
        </script>
      <!--mathjax-->
        <script
          src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
        ></script>
        <script>
          MathJax.Hub.Config({
              jax: ["input/TeX","output/HTML-CSS"],
              displayAlign: "left",
              displayIndent: "20px"
          });
        </script>
      <!--google analytics-->
        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-47867706-1', 'auto');
          ga('require', 'displayfeatures');
          ga('send', 'pageview');

        </script>
  </body>
</html>
